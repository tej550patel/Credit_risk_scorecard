{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scorecardpy as sc\n",
    "import EScorecard as es\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('data/final_features.csv')\n",
    "train_flag = pd.read_csv('data/train/train_flag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_all, train_flag[['uid','TARGET']], on='uid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>total_overdue</th>\n",
       "      <th>avg_overdue</th>\n",
       "      <th>max_overdue</th>\n",
       "      <th>min_overdue</th>\n",
       "      <th>std_overdue</th>\n",
       "      <th>num_payments</th>\n",
       "      <th>max_recent_overdue</th>\n",
       "      <th>overall_overdue_trend</th>\n",
       "      <th>max_consecutive_on_time</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_revolving_loans_enquiry</th>\n",
       "      <th>sum_unknown_type_of_loan_enquiry</th>\n",
       "      <th>avg_loan_duration</th>\n",
       "      <th>total_loan_duration</th>\n",
       "      <th>longest_loan_duration</th>\n",
       "      <th>shortest_loan_duration</th>\n",
       "      <th>loans_still_open</th>\n",
       "      <th>proportion_open_loans</th>\n",
       "      <th>days_since_last_loan</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAA09044550</td>\n",
       "      <td>68</td>\n",
       "      <td>2.106618</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>6.656192</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>94000</td>\n",
       "      <td>98000</td>\n",
       "      <td>511.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAA10545297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>99000</td>\n",
       "      <td>134000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAA14112888</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAA20326915</td>\n",
       "      <td>14477</td>\n",
       "      <td>23.810855</td>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "      <td>35.803400</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>387.4</td>\n",
       "      <td>1937.0</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAA31604840</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>184000</td>\n",
       "      <td>463.5</td>\n",
       "      <td>927.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid  total_overdue  avg_overdue  max_overdue  min_overdue  \\\n",
       "0  AAA09044550             68     2.106618           44            0   \n",
       "1  AAA10545297              0     0.000000            0            0   \n",
       "2  AAA14112888              0     0.000000            0            0   \n",
       "3  AAA20326915          14477    23.810855          917            0   \n",
       "4  AAA31604840              0     0.000000            0            0   \n",
       "\n",
       "   std_overdue  num_payments  max_recent_overdue  overall_overdue_trend  \\\n",
       "0     6.656192            33                   0                      0   \n",
       "1     0.000000             6                   0                      0   \n",
       "2     0.000000             3                   0                      0   \n",
       "3    35.803400           151                   0                      0   \n",
       "4     0.000000            78                   0                      0   \n",
       "\n",
       "   max_consecutive_on_time  ...  sum_revolving_loans_enquiry  \\\n",
       "0                        9  ...                        94000   \n",
       "1                        6  ...                        99000   \n",
       "2                        3  ...                            0   \n",
       "3                       48  ...                            0   \n",
       "4                       43  ...                            0   \n",
       "\n",
       "   sum_unknown_type_of_loan_enquiry  avg_loan_duration  total_loan_duration  \\\n",
       "0                             98000              511.0               1022.0   \n",
       "1                            134000                NaN                  0.0   \n",
       "2                             17000               92.0                 92.0   \n",
       "3                                 0              387.4               1937.0   \n",
       "4                            184000              463.5                927.0   \n",
       "\n",
       "   longest_loan_duration  shortest_loan_duration  loans_still_open  \\\n",
       "0                  518.0                   504.0                 0   \n",
       "1                    NaN                     NaN                 1   \n",
       "2                   92.0                    92.0                 0   \n",
       "3                 1450.0                    31.0                 3   \n",
       "4                  549.0                   378.0                 3   \n",
       "\n",
       "   proportion_open_loans  days_since_last_loan TARGET  \n",
       "0                  0.000                  2236      0  \n",
       "1                  1.000                  1590      0  \n",
       "2                  0.000                  1610      0  \n",
       "3                  0.375                  1591      0  \n",
       "4                  0.600                  1487      0  \n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07706839110745899"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['TARGET'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate WoE and IV for a single feature\n",
    "def calculate_woe_iv(df, feature, target):\n",
    "    # Create bins for the feature if it's continuous\n",
    "    df[feature+'_bin'] = pd.qcut(df[feature].rank(method=\"first\"), q=10, duplicates=\"drop\")\n",
    "    # Calculate the number of events (bad loans) and non-events (good loans) in each bin\n",
    "    grouped = df.groupby(feature+'_bin')[target].agg(['count', 'sum'])\n",
    "    grouped.columns = ['Total', 'Bad']\n",
    "    grouped['Good'] = grouped['Total'] - grouped['Bad']\n",
    "    # Avoid division by zero\n",
    "    grouped['Bad'] = grouped['Bad'].replace(0, 0.5)\n",
    "    grouped['Good'] = grouped['Good'].replace(0, 0.5)\n",
    "    # Calculate distribution of events and non-events\n",
    "    grouped['Bad_dist'] = grouped['Bad'] / grouped['Bad'].sum()\n",
    "    grouped['Good_dist'] = grouped['Good'] / grouped['Good'].sum()\n",
    "    \n",
    "    grouped['WoE'] = np.log(grouped['Good_dist'] / grouped['Bad_dist'])\n",
    "    grouped['IV'] = (grouped['Good_dist'] - grouped['Bad_dist']) * grouped['WoE']\n",
    "    iv = grouped['IV'].sum()\n",
    "    \n",
    "    return iv, grouped[['WoE', 'IV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each feature and calculate IV\n",
    "iv_dict = {}\n",
    "for col in df_all.columns:\n",
    "    if col != 'TARGET':  \n",
    "        iv, woe_table = calculate_woe_iv(df_all, col, 'TARGET')\n",
    "        iv_dict[col] = iv\n",
    "\n",
    "df_IV = pd.DataFrame(list(iv_dict.items()), columns=['Feature', 'IV']).sort_values(by='IV', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 60 features wrt IV\n",
    "top_60 = list(df_IV.head(60).Feature.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_all[['uid'] + top_60 + ['TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_filtered[top_60].corr().abs()\n",
    "iv_dict = df_IV.set_index('Feature')['IV'].to_dict()  \n",
    "\n",
    "features_to_drop = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the correlation matrix to find highly correlated pairs\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)):\n",
    "        feature_1 = corr_matrix.columns[i]\n",
    "        feature_2 = corr_matrix.columns[j]\n",
    "        \n",
    "        # Check if correlation is above 0.5 and neither feature is already flagged to be dropped\n",
    "        if corr_matrix.iloc[i, j] > 0.5 and feature_1 not in features_to_drop and feature_2 not in features_to_drop:\n",
    "            # Compare IV values and select the feature to keep\n",
    "            if iv_dict[feature_1] > iv_dict[feature_2]:\n",
    "                features_to_drop.add(feature_2)\n",
    "            else:\n",
    "                features_to_drop.add(feature_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features = [feature for feature in top_60 if feature not in features_to_drop]\n",
    "df = df_filtered[['uid'] + filtered_features + ['TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'TARGET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('missing', np.nan, inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['TARGET'])\n",
    "y = df['TARGET']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1.0,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='logloss', gamma=0.2, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.01, max_bin=256,\n",
       "              max_cat_to_onehot=4, max_delta_step=0, max_depth=4, max_leaves=0,\n",
       "              min_child_weight=2, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=0.1, ...)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These parameters are gained after randomized search performed bwloe\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    subsample=1.0,\n",
    "    reg_lambda=0.1,\n",
    "    reg_alpha=0,\n",
    "    n_estimators=100,\n",
    "    min_child_weight=2,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    gamma=0.2,\n",
    "    colsample_bytree=1.0,\n",
    "    use_label_encoder=False,  \n",
    "    eval_metric='logloss'    \n",
    ")\n",
    "\n",
    "\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = xgb_model.predict_proba(X_train)[:, 1]  \n",
    "y_val_pred = xgb_model.predict_proba(X_val)[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "val_auc = roc_auc_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.6332\n",
      "Validation AUC: 0.6244\n"
     ]
    }
   ],
   "source": [
    "print(f'Training AUC: {train_auc:.4f}')\n",
    "print(f'Validation AUC: {val_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 4, 5, 6],\n",
    "    'classifier__min_child_weight': [1, 2, 5],\n",
    "    'classifier__gamma': [0, 0.1, 0.2],\n",
    "    'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1],\n",
    "    'classifier__reg_lambda': [0.1, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the randomized search with cross-validation\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# random_search = RandomizedSearchCV(model_xgb, param_distributions=param_grid, \n",
    "#                                    n_iter=100, scoring='roc_auc', \n",
    "#                                    cv=5, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=50, scoring='roc_auc', \n",
    "                                   cv=3, verbose=2, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[18:33:33] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__learning_rate\", \"classifier__max_depth\", \"classifier__min_child_weight\", \"classifier__n_estimators\", \"classifier__reg_alpha\", \"classifier__reg_lambda\", \"classifier__subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           callbacks=None, colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric='logloss', gamma=0,\n",
       "                                           gpu_id=-1, grow_policy='depthwise',\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_bin...\n",
       "                                                                         1.0],\n",
       "                                        'classifier__gamma': [0, 0.1, 0.2],\n",
       "                                        'classifier__learning_rate': [0.01,\n",
       "                                                                      0.05,\n",
       "                                                                      0.1],\n",
       "                                        'classifier__max_depth': [3, 4, 5, 6],\n",
       "                                        'classifier__min_child_weight': [1, 2,\n",
       "                                                                         5],\n",
       "                                        'classifier__n_estimators': [100, 200,\n",
       "                                                                     300],\n",
       "                                        'classifier__reg_alpha': [0, 0.01, 0.1],\n",
       "                                        'classifier__reg_lambda': [0.1, 1, 10],\n",
       "                                        'classifier__subsample': [0.6, 0.8,\n",
       "                                                                  1.0]},\n",
       "                   random_state=42, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'classifier__subsample': 1.0, 'classifier__reg_lambda': 0.1, 'classifier__reg_alpha': 0, 'classifier__n_estimators': 100, 'classifier__min_child_weight': 2, 'classifier__max_depth': 4, 'classifier__learning_rate': 0.01, 'classifier__gamma': 0.2, 'classifier__colsample_bytree': 1.0}\n",
      "Best cross-validation AUC:  0.6213497704039089\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best cross-validation AUC: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC after hyperparameter tuning: 0.6286\n"
     ]
    }
   ],
   "source": [
    "print(f'Validation AUC after hyperparameter tuning: {val_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/final_test_features.csv')\n",
    "test_flag = pd.read_csv('data/test/test_flag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.set_index('uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[filtered_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.replace('missing', np.nan, inplace=True)\n",
    "df_test = df_test.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['predictions'] = xgb_model.predict(df_test)\n",
    "\n",
    "df_test['pred'] = xgb_model.predict_proba(df_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index()[['uid','pred']].to_csv('Tej_Patel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
